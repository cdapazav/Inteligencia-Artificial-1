{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86415a6a",
   "metadata": {},
   "source": [
    "# **Primer parcial**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aba300c",
   "metadata": {},
   "source": [
    "## Universitario: Apaza Villca Cristofer Denilson  \n",
    "\n",
    "**CU: 35 - 4436  Ing. de Sistemas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a4d160",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "## 1.   Importación del **Dataset**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ed031dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249d9fc9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "## 2.   Carga de Dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7161848a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones: (204876, 8)\n",
      "\n",
      "Primeras filas:\n",
      "   id        date  state_code        state_name  actual       rfs  normal  \\\n",
      "0   0  2009-01-01           5       Uttarakhand     0.0  0.003906    2.19   \n",
      "1   1  2009-01-01          18             Assam     0.0  0.000000    0.52   \n",
      "2   2  2009-01-01          16           Tripura     0.0  0.000000    0.09   \n",
      "3   3  2009-01-01          36         Telangana     0.0  0.000000    0.17   \n",
      "4   4  2009-01-01           2  Himachal Pradesh     0.0  0.008566    3.31   \n",
      "\n",
      "   deviation  \n",
      "0     -100.0  \n",
      "1     -100.0  \n",
      "2     -100.0  \n",
      "3     -100.0  \n",
      "4     -100.0  \n"
     ]
    }
   ],
   "source": [
    "# ---------- 1. Cargar dataset ----------\n",
    "# Ajusta el nombre del archivo según tu CSV real\n",
    "filepath = \"C:\\IA_PROJECTS\\PARCIALES\\PARCIAL1\\daily-rainfall-at-state-level.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "print(\"Dimensiones:\", df.shape)\n",
    "print(\"\\nPrimeras filas:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4224585",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11160ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conteo de valores nulos por columna:\n",
      "id                0\n",
      "date              0\n",
      "state_code        0\n",
      "state_name        0\n",
      "actual        17162\n",
      "rfs            5865\n",
      "normal        11518\n",
      "deviation     31021\n",
      "dtype: int64\n",
      "\n",
      "Tipos de datos:\n",
      "id              int64\n",
      "date           object\n",
      "state_code      int64\n",
      "state_name     object\n",
      "actual        float64\n",
      "rfs           float64\n",
      "normal        float64\n",
      "deviation     float64\n",
      "dtype: object\n",
      "\n",
      "Columnas de tipo texto: ['date', 'state_name']\n"
     ]
    }
   ],
   "source": [
    "# ---------- 2. Revisar datos incompletos ----------\n",
    "print(\"\\nConteo de valores nulos por columna:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Revisar tipos de datos\n",
    "print(\"\\nTipos de datos:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Identificar columnas de texto\n",
    "text_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"\\nColumnas de tipo texto:\", text_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63d7f9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna actual: NaN reemplazados con mediana 0.15\n",
      "Columna rfs: NaN reemplazados con mediana 0.063856\n",
      "Columna normal: NaN reemplazados con mediana 1.87\n",
      "Columna deviation: NaN reemplazados con mediana -89.06\n",
      "\n",
      "Nulos restantes después de imputación:\n",
      "id            0\n",
      "date          0\n",
      "state_code    0\n",
      "state_name    0\n",
      "actual        0\n",
      "rfs           0\n",
      "normal        0\n",
      "deviation     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ---------- 3. Completar datos incompletos ----------\n",
    "# Estrategia:\n",
    "# - numéricas: rellenar con mediana\n",
    "# - texto: rellenar con valor más frecuente (mode)\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].isna().sum() > 0:  # solo columnas con NA\n",
    "        if df[col].dtype in [np.float64, np.int64]:\n",
    "            mediana = df[col].median()\n",
    "            df[col] = df[col].fillna(mediana)  # asignación directa\n",
    "            print(f\"Columna {col}: NaN reemplazados con mediana {mediana}\")\n",
    "        else:\n",
    "            moda = df[col].mode()[0]\n",
    "            df[col] = df[col].fillna(moda)  # asignación directa\n",
    "            print(f\"Columna {col}: NaN reemplazados con moda '{moda}'\")\n",
    "\n",
    "# Verificación\n",
    "print(\"\\nNulos restantes después de imputación:\")\n",
    "print(df.isna().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "217ac336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'date' convertida a datetime (solo fecha, sin hora).\n",
      "\n",
      "Primeras filas después de reemplazar state_name por state_code:\n",
      "   state_name  state_code\n",
      "0           5           5\n",
      "1          18          18\n",
      "2          16          16\n",
      "3          36          36\n",
      "4           2           2\n",
      "\n",
      "Tipos de datos después de conversiones:\n",
      "id                     int64\n",
      "date          datetime64[ns]\n",
      "state_code             int64\n",
      "state_name             int64\n",
      "actual               float64\n",
      "rfs                  float64\n",
      "normal               float64\n",
      "deviation            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# ---------- 4. Convertir texto a números ----------\n",
    "\n",
    "# 1️⃣ Convertir 'date' a datetime y mantener solo fecha (sin hora)\n",
    "if 'date' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.normalize()\n",
    "    print(\"Columna 'date' convertida a datetime (solo fecha, sin hora).\")\n",
    "\n",
    "# 2️⃣ Reemplazar 'state_name' por su código numérico\n",
    "if 'state_name' in df.columns and 'state_code' in df.columns:\n",
    "    mapping = dict(zip(df['state_name'], df['state_code']))\n",
    "    df['state_name'] = df['state_name'].map(mapping).astype(int)\n",
    "    print(\"\\nPrimeras filas después de reemplazar state_name por state_code:\")\n",
    "    print(df[['state_name', 'state_code']].head())\n",
    "\n",
    "# 3️⃣ Convertir otras columnas de texto a códigos numéricos\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    if col not in ['state_name', 'date']:\n",
    "        df[col] = df[col].astype('category').cat.codes\n",
    "        print(f\"Columna {col} convertida a códigos numéricos\")\n",
    "\n",
    "# 4️⃣ Verificación final de tipos de datos\n",
    "print(\"\\nTipos de datos después de conversiones:\")\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "51245124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tipos de datos después de transformaciones:\n",
      "id                     int64\n",
      "date          datetime64[ns]\n",
      "state_code             int64\n",
      "state_name             int64\n",
      "actual               float64\n",
      "rfs                  float64\n",
      "normal               float64\n",
      "deviation            float64\n",
      "dtype: object\n",
      "\n",
      "Archivo limpio guardado como rainfall_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------- 5. Resultado final ----------\n",
    "print(\"\\nTipos de datos después de transformaciones:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Guardar dataset limpio\n",
    "df.to_csv(\"rainfall_clean.csv\", index=False)\n",
    "print(\"\\nArchivo limpio guardado como rainfall_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d366dc07",
   "metadata": {},
   "source": [
    "### Cargar el dataset Limpio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "06395735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1. Leer el archivo CSV ----------\n",
    "df_clean = pd.read_csv(\"rainfall_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "359243f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset original: (204876, 8)\n",
      "Dimensiones del dataset leído: (204876, 8)\n"
     ]
    }
   ],
   "source": [
    "# ---------- 2. Revisar dimensiones ----------\n",
    "print(\"Dimensiones del dataset original:\", df.shape)\n",
    "print(\"Dimensiones del dataset leído:\", df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "112e197b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tipos de datos del dataset original:\n",
      "id                     int64\n",
      "date          datetime64[ns]\n",
      "state_code             int64\n",
      "state_name             int64\n",
      "actual               float64\n",
      "rfs                  float64\n",
      "normal               float64\n",
      "deviation            float64\n",
      "dtype: object\n",
      "\n",
      "Tipos de datos del dataset leído desde CSV:\n",
      "id              int64\n",
      "date           object\n",
      "state_code      int64\n",
      "state_name      int64\n",
      "actual        float64\n",
      "rfs           float64\n",
      "normal        float64\n",
      "deviation     float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# ---------- 3. Verificar tipos de datos ----------\n",
    "print(\"\\nTipos de datos del dataset original:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nTipos de datos del dataset leído desde CSV:\")\n",
    "print(df_clean.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b56b0454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "¿El dataset leído es exactamente igual al original? False\n"
     ]
    }
   ],
   "source": [
    "# ---------- 4. Revisar si los datos son iguales ----------\n",
    "# Esto compara si cada celda del DataFrame original y leído es exactamente igual\n",
    "igualdad = df.equals(df_clean)\n",
    "print(\"\\n¿El dataset leído es exactamente igual al original?\", igualdad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a30c58e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primeras filas del dataset leído:\n",
      "   id        date  state_code  state_name  actual       rfs  normal  deviation\n",
      "0   0  1970-01-01           5           5     0.0  0.003906    2.19     -100.0\n",
      "1   1  1970-01-01          18          18     0.0  0.000000    0.52     -100.0\n",
      "2   2  1970-01-01          16          16     0.0  0.000000    0.09     -100.0\n",
      "3   3  1970-01-01          36          36     0.0  0.000000    0.17     -100.0\n",
      "4   4  1970-01-01           2           2     0.0  0.008566    3.31     -100.0\n"
     ]
    }
   ],
   "source": [
    "# ---------- 5. Opcional: revisar primeras filas ----------\n",
    "print(\"\\nPrimeras filas del dataset leído:\")\n",
    "print(df_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f59a308",
   "metadata": {},
   "source": [
    "dividimos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a3f456a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño total: 204876\n",
      "Tamaño train: 163900\n",
      "Tamaño test: 40976\n",
      "\n",
      "Primeras filas de train:\n",
      "            id       date  state_code  state_name  actual         rfs  normal  \\\n",
      "0            0 1970-01-01           5           5    0.00    0.003906    2.19   \n",
      "136578  136578 1970-01-01          27          27   17.67  185.797279    5.70   \n",
      "136579  136579 1970-01-01           3           3    0.02    0.033211    5.06   \n",
      "136580  136580 1970-01-01          20          20    7.71   20.911486    9.16   \n",
      "136581  136581 1970-01-01          28          28    2.67   14.824959    4.49   \n",
      "\n",
      "        deviation  \n",
      "0         -100.00  \n",
      "136578     210.00  \n",
      "136579     -99.60  \n",
      "136580     -15.83  \n",
      "136581     -40.53  \n",
      "\n",
      "Primeras filas de test:\n",
      "          id       date  state_code  state_name  actual        rfs  normal  \\\n",
      "81946  81946 1970-01-01          28          28    0.14   0.771389    1.79   \n",
      "81947  81947 1970-01-01          12          12   13.88  38.882092   12.39   \n",
      "81948  81948 1970-01-01          37          37    4.49  12.671121    1.87   \n",
      "81949  81949 1970-01-01          35          35    0.15   0.000000   14.29   \n",
      "81950  81950 1970-01-01          30          30    0.00   0.000000    5.20   \n",
      "\n",
      "       deviation  \n",
      "81946     -92.18  \n",
      "81947      12.03  \n",
      "81948     -89.06  \n",
      "81949     -89.06  \n",
      "81950    -100.00  \n"
     ]
    }
   ],
   "source": [
    "# Asegúrate que el DataFrame esté ordenado por fecha\n",
    "df.sort_values(by='date', inplace=True)\n",
    "\n",
    "# Parámetro: porcentaje de datos para entrenamiento\n",
    "train_ratio = 0.8\n",
    "train_size = int(len(df) * train_ratio)\n",
    "\n",
    "# Dividir en train y test\n",
    "train_df = df.iloc[:train_size].copy()\n",
    "test_df = df.iloc[train_size:].copy()\n",
    "\n",
    "# Verificación de tamaños\n",
    "print(f\"Tamaño total: {len(df)}\")\n",
    "print(f\"Tamaño train: {len(train_df)}\")\n",
    "print(f\"Tamaño test: {len(test_df)}\")\n",
    "\n",
    "# Primeras filas de train y test\n",
    "print(\"\\nPrimeras filas de train:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nPrimeras filas de test:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f0ab8",
   "metadata": {},
   "source": [
    "Normalizamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0fa8c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  featureNormalize(X):\n",
    "    X_norm = X.copy()\n",
    "    mu = np.zeros(X.shape[1])\n",
    "    sigma = np.zeros(X.shape[1])\n",
    "\n",
    "    mu = np.mean(X, axis = 0)\n",
    "    sigma = np.std(X, axis = 0)\n",
    "    X_norm = (X - mu) / sigma\n",
    "\n",
    "    return X_norm, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "20c8f3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas normalizadas de train:\n",
      "              id       date  state_code  state_name    actual       rfs  \\\n",
      "0      -1.711588 1970-01-01   -1.271899   -1.271899 -0.392637 -0.342007   \n",
      "136578  0.427839 1970-01-01    0.684717    0.684717  1.504982  6.592618   \n",
      "136579  0.427855 1970-01-01   -1.449773   -1.449773 -0.390489 -0.340913   \n",
      "136580  0.427871 1970-01-01    0.062158    0.062158  0.435357  0.438356   \n",
      "136581  0.427886 1970-01-01    0.773655    0.773655 -0.105900  0.211180   \n",
      "\n",
      "          normal  deviation  \n",
      "0      -0.317668  -0.088788  \n",
      "136578  0.173301   0.137400  \n",
      "136579  0.083780  -0.088496  \n",
      "136580  0.657277  -0.027374  \n",
      "136581  0.004049  -0.045396  \n",
      "\n",
      "Primeras filas normalizadas de test:\n",
      "             id       date  state_code  state_name    actual       rfs  \\\n",
      "81946 -0.427944 1970-01-01    0.773655    0.773655 -0.377602 -0.313361   \n",
      "81947 -0.427928 1970-01-01   -0.649339   -0.649339  1.097966  1.109098   \n",
      "81948 -0.427913 1970-01-01    1.574088    1.574088  0.089554  0.130789   \n",
      "81949 -0.427897 1970-01-01    1.396214    1.396214 -0.376528 -0.342153   \n",
      "81950 -0.427881 1970-01-01    0.951529    0.951529 -0.392637 -0.342153   \n",
      "\n",
      "         normal  deviation  \n",
      "81946 -0.373619  -0.083082  \n",
      "81947  1.109081  -0.007047  \n",
      "81948 -0.362429  -0.080806  \n",
      "81949  1.374848  -0.080806  \n",
      "81950  0.103362  -0.088788  \n"
     ]
    }
   ],
   "source": [
    "# Columnas a normalizar\n",
    "cols_to_normalize = ['id','state_code','state_name','actual', 'rfs', 'normal','deviation']\n",
    "\n",
    "# ---------- 1️⃣ Normalizar train ----------\n",
    "X_train = train_df[cols_to_normalize].values\n",
    "X_train_norm, mu, sigma = featureNormalize(X_train)\n",
    "\n",
    "# Guardamos de nuevo en train_df\n",
    "train_df[cols_to_normalize] = X_train_norm\n",
    "\n",
    "# ---------- 2️⃣ Normalizar test usando mu y sigma de train ----------\n",
    "X_test = test_df[cols_to_normalize].values\n",
    "X_test_norm = (X_test - mu) / sigma\n",
    "test_df[cols_to_normalize] = X_test_norm\n",
    "\n",
    "# ---------- 3️⃣ Verificación ----------\n",
    "print(\"Primeras filas normalizadas de train:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nPrimeras filas normalizadas de test:\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115b7270",
   "metadata": {},
   "source": [
    "## Construccion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "de569b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X_train: (163870, 30)\n",
      "Shape y_train: (163870,)\n",
      "Shape X_test: (40946, 30)\n",
      "Shape y_test: (40946,)\n",
      "\n",
      "Shape X_train listo para LSTM: (163870, 30, 1)\n",
      "Tipos de datos:\n",
      "float32 float32 float32 float32\n"
     ]
    }
   ],
   "source": [
    "# Número de días que usamos como ventana\n",
    "window_size = 30\n",
    "\n",
    "def create_sequences(data, window_size):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i+window_size])\n",
    "        y.append(data[i+window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# ---------- 1️⃣ Crear secuencias para train ----------\n",
    "train_values = train_df['rfs'].values\n",
    "X_train_seq, y_train_seq = create_sequences(train_values, window_size)\n",
    "\n",
    "# ---------- 2️⃣ Crear secuencias para test ----------\n",
    "test_values = test_df['rfs'].values\n",
    "X_test_seq, y_test_seq = create_sequences(test_values, window_size)\n",
    "\n",
    "# ---------- 3️⃣ Revisar shapes ----------\n",
    "print(\"Shape X_train:\", X_train_seq.shape)  # (num_samples, window_size)\n",
    "print(\"Shape y_train:\", y_train_seq.shape)  # (num_samples,)\n",
    "print(\"Shape X_test:\", X_test_seq.shape)\n",
    "print(\"Shape y_test:\", y_test_seq.shape)\n",
    "\n",
    "# ---------- 4️⃣ Adaptar para LSTM (agregar dimensión de features) ----------\n",
    "# LSTM espera input de shape (samples, timesteps, features)\n",
    "X_train_seq = X_train_seq.reshape((X_train_seq.shape[0], X_train_seq.shape[1], 1))\n",
    "X_test_seq = X_test_seq.reshape((X_test_seq.shape[0], X_test_seq.shape[1], 1))\n",
    "\n",
    "# ---------- 5️⃣ Convertir a float32 (muy importante para keras LSTM) ----------\n",
    "X_train_seq = X_train_seq.astype('float32')\n",
    "y_train_seq = y_train_seq.astype('float32')\n",
    "X_test_seq = X_test_seq.astype('float32')\n",
    "y_test_seq = y_test_seq.astype('float32')\n",
    "\n",
    "print(\"\\nShape X_train listo para LSTM:\", X_train_seq.shape)\n",
    "print(\"Tipos de datos:\")\n",
    "print(X_train_seq.dtype, y_train_seq.dtype, X_test_seq.dtype, y_test_seq.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a3b342c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The filepath provided must end in `.keras` (Keras model format). Received: filepath=best_lstm_model.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n\u001b[32m     11\u001b[39m early_stop = EarlyStopping(monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, patience=\u001b[32m10\u001b[39m, restore_best_weights=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m checkpoint = \u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbest_lstm_model.h5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mval_loss\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_best_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Entrenar\u001b[39;00m\n\u001b[32m     15\u001b[39m history = model.fit(\n\u001b[32m     16\u001b[39m     X_train_seq, y_train_seq,\n\u001b[32m     17\u001b[39m     epochs=\u001b[32m100\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     verbose=\u001b[32m2\u001b[39m\n\u001b[32m     22\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hpOmen\\anaconda3\\envs\\ML\\Lib\\site-packages\\keras\\src\\callbacks\\model_checkpoint.py:191\u001b[39m, in \u001b[36mModelCheckpoint.__init__\u001b[39m\u001b[34m(self, filepath, monitor, verbose, save_best_only, save_weights_only, mode, save_freq, initial_value_threshold)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.filepath.endswith(\u001b[33m\"\u001b[39m\u001b[33m.keras\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    192\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe filepath provided must end in `.keras` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    193\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m(Keras model format). Received: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    194\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfilepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.filepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    195\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: The filepath provided must end in `.keras` (Keras model format). Received: filepath=best_lstm_model.h5"
     ]
    }
   ],
   "source": [
    "# ---------- Definir modelo ----------\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='tanh', input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compilar\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_lstm_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Entrenar\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop, checkpoint],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Evaluar\n",
    "loss_train = model.evaluate(X_train_seq, y_train_seq, verbose=0)\n",
    "loss_test = model.evaluate(X_test_seq, y_test_seq, verbose=0)\n",
    "print(f\"\\nMSE Train: {loss_train:.4f}, MSE Test: {loss_test:.4f}\")\n",
    "\n",
    "# Predicción\n",
    "y_pred = model.predict(X_test_seq)\n",
    "print(\"\\nPrimeras 5 predicciones:\", y_pred[:5].flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4eb21",
   "metadata": {},
   "source": [
    "## Graficar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb37264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
